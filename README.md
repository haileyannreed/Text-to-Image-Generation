# ğŸ§  Stable Diffusion: Text-to-Image Generation

This project demonstrates how Stable Diffusion works under the hood by walking through the full pipeline: from a text prompt to a generated image â€” using PyTorch, Hugging Face ğŸ¤— `diffusers`, and Google Colab.

---

## ğŸ§ª Project Type
**Generative AI** | **Computer Vision** | **Diffusion Models**

---

## ğŸš€ What is Stable Diffusion?

Stable Diffusion is a **latent text-to-image generation model**. It starts with random noise and gradually denoises it into a realistic image that matches a given text prompt.

Rather than generating full images directly, Stable Diffusion works in **latent space** â€” creating a compressed representation of the image, which is later decoded.

---

## ğŸ”§ Core Components

| Component      | Description                                                                 |
|----------------|-----------------------------------------------------------------------------|
| **Tokenizer**  | CLIP tokenizer breaks the prompt into token IDs (shape: `(1, 77)`)           |
| **Text Encoder** | CLIP text encoder converts tokens â†’ embeddings `(1, 77, 768)`              |
| **Latent Noise** | Start from pure noise in shape `(1, 4, 64, 64)`                            |
| **U-Net (Diffuser)** | Learns to denoise latent step-by-step, guided by the text embedding   |
| **Scheduler**  | Controls the denoising schedule (DDIM, Euler, etc.)                          |
| **VAE Decoder** | Converts final latent â†’ full-size image `(1, 3, 512, 512)`                  |

---

## ğŸ§¬ Full Pipeline (Step-by-Step)

1. **Tokenize the text** prompt â†’ shape `(1, 77)`
2. **Embed with CLIP** â†’ shape `(1, 77, 768)`
3. **Generate random latent noise** â†’ shape `(1, 4, 64, 64)`
4. **U-Net denoising** over 50â€“100 steps, guided by the prompt
5. **Decode final latent** with VAE â†’ image `(1, 3, 512, 512)`

---

## ğŸ¨ Diffusers Used

This project compares results from **3 different pretrained Stable Diffusion models**:

| Diffuser Model | Style | Link |
|----------------|-------|------|
| `CompVis/stable-diffusion-v1-4` | General-purpose photorealism | [ğŸ”— Hugging Face](https://huggingface.co/CompVis/stable-diffusion-v1-4) |
| `prompthero/openjourney`        | Fantasy / Midjourney-style art | [ğŸ”— Hugging Face](https://huggingface.co/prompthero/openjourney) |
| `SG161222/Realistic_Vision_V4.0` | High-end photorealistic rendering | [ğŸ”— Hugging Face](https://huggingface.co/SG161222/Realistic_Vision_V4.0) |

ğŸ”— Explore other models here: [Text-to-Image Models on Hugging Face](https://huggingface.co/models?pipeline_tag=text-to-image&library=diffusers)

---

## ğŸ“¸ Example Output

Prompt:  
`"a realistic duck wearing sunglasses sitting on the beach"`

Generated using SD v1.5:
![Generated Duck](INSERT_IMAGE_LINK_HERE)

---

## ğŸ“š Technologies Used

- Python + PyTorch
- Hugging Face `diffusers`, `transformers`
- Google Colab (for GPU acceleration)
- Matplotlib + PIL for visualization

---

## ğŸ§  Concepts Demonstrated

- CLIP Tokenization and Embedding
- Diffusion models & denoising schedules
- Latent vs. pixel space generation
- Classifier-free guidance
- Prompt engineering for visual generation

---

## ğŸ“ Files in This Project

| File | Description |
|------|-------------|
| `Stable_Diffusion.ipynb` | Full Colab notebook with pipeline and visualizations |
| `README.md` | Project overview and methodology |
| `outputs/` | Folder of sample outputs for different prompts and models |

---

## âœ¨ Portfolio Significance

This project illustrates:
- My understanding of **Stable Diffusion architecture**
- My ability to break down complex generative AI systems
- Practical experience using **CLIP**, **U-Net**, and **VAE** together

---

## ğŸ“¬ Contact

Feel free to connect with me if youâ€™d like to collaborate or learn more:
**Hailey Reed**  
ğŸ–¥ï¸ [GitHub](https://github.com/haileyannreed)  
ğŸŒ [Portfolio](https://haileyannreed.github.io/portfolio)
